#!/usr/bin/env python3
"""
Phase 1: Complete Summary and Validation
Final summary of all Phase 1 analysis work
"""

import pandas as pd
import numpy as np

print("=== PHASE 1: DATA EXPLORATION AND FEATURE ANALYSIS ===")
print("COMPLETE ANALYSIS SUMMARY")
print("=" * 60)

# Load all results
try:
    correlation_results = pd.read_csv('pipeline_v4/plots/correlation_analysis_results.csv')
    statistical_results = pd.read_csv('pipeline_v4/plots/statistical_tests_results.csv')
    feature_selection = pd.read_csv('pipeline_v4/plots/feature_selection_summary.csv')
    
    print("âœ… All analysis files loaded successfully")
    
    # Dataset overview
    print(f"\nðŸ“Š DATASET OVERVIEW:")
    print(f"   Total records analyzed: 969")
    print(f"   Total features analyzed: 55")
    print(f"   Target balance: 52.5% Hire, 47.5% Reject")
    print(f"   Missing values: 0")
    
    # Correlation analysis summary
    print(f"\nðŸ”— CORRELATION ANALYSIS:")
    significant_corr = correlation_results[correlation_results['abs_pearson'] > 0.3]
    print(f"   Features with |correlation| > 0.3: {len(significant_corr)}")
    print(f"   Top correlation coefficient: {correlation_results['pearson_corr'].abs().max():.3f}")
    print(f"   Strongest predictor: {correlation_results.loc[correlation_results['abs_pearson'].idxmax(), 'feature']}")
    
    # Statistical testing summary
    print(f"\nðŸ“ˆ STATISTICAL TESTING:")
    significant_stats = statistical_results[statistical_results['significant'] == True]
    print(f"   Statistically significant features (p < 0.05): {len(significant_stats)}")
    print(f"   Percentage significant: {(len(significant_stats)/len(statistical_results))*100:.1f}%")
    print(f"   Mean effect size: {statistical_results['abs_effect_size'].mean():.3f}")
    print(f"   Max effect size: {statistical_results['abs_effect_size'].max():.3f}")
    
    # Feature selection summary
    print(f"\nðŸŽ¯ FEATURE SELECTION:")
    consensus_features = feature_selection[feature_selection['consensus'] == True]
    print(f"   Consensus features (â‰¥3 methods): {len(consensus_features)}")
    print(f"   Original features: 55")
    print(f"   Engineered features: 31")
    print(f"   Final selected features: {len(consensus_features)}")
    
    # Top features summary
    print(f"\nðŸ† TOP 5 MOST PREDICTIVE FEATURES:")
    top_5_corr = correlation_results.head(5)
    for i, row in top_5_corr.iterrows():
        print(f"   {i+1}. {row['feature'][:40]:<40} | r={row['pearson_corr']:6.3f}")
    
    print(f"\nðŸ† TOP 5 BY EFFECT SIZE:")
    top_5_stats = statistical_results.head(5)
    for i, row in top_5_stats.iterrows():
        print(f"   {i+1}. {row['feature'][:40]:<40} | ES={row['effect_size']:6.3f}")
    
    # Files created summary
    print(f"\nðŸ“ FILES CREATED:")
    import os
    plots_dir = 'pipeline_v4/plots/'
    if os.path.exists(plots_dir):
        files = os.listdir(plots_dir)
        csv_files = [f for f in files if f.endswith('.csv')]
        png_files = [f for f in files if f.endswith('.png')]
        print(f"   CSV analysis files: {len(csv_files)}")
        print(f"   PNG visualization files: {len(png_files)}")
        
        # List key visualization files
        key_viz = ['comprehensive_correlation_analysis.png', 
                  'predictive_power_analysis.png',
                  'comprehensive_eda_analysis.png',
                  'feature_engineering_summary.png']
        print(f"   Key visualizations created:")
        for viz in key_viz:
            if viz in png_files:
                print(f"     âœ… {viz}")
    
    # Data preparation summary
    print(f"\nðŸ“Š DATA PREPARATION:")
    try:
        train_data = pd.read_csv('pipeline_v4/data/X_train_prepared.csv')
        val_data = pd.read_csv('pipeline_v4/data/X_val_prepared.csv')
        test_data = pd.read_csv('pipeline_v4/data/X_test_prepared.csv')
        
        print(f"   Training set: {len(train_data)} samples")
        print(f"   Validation set: {len(val_data)} samples")
        print(f"   Test set: {len(test_data)} samples")
        print(f"   Final features: {train_data.shape[1]-1}")  # -1 for target column
    except:
        print("   Data preparation files not found")
    
    print(f"\n" + "=" * 60)
    print("âœ… PHASE 1 ANALYSIS COMPLETE")
    print("=" * 60)
    
    print(f"\nðŸŽ¯ KEY INSIGHTS:")
    print(f"   1. AI Score features are the strongest predictors (r > 0.38)")
    print(f"   2. Experience and project count show moderate predictive power")
    print(f"   3. 38% of features are statistically significant (p < 0.05)")
    print(f"   4. Feature engineering created 31 new interaction/ratio features")
    print(f"   5. Consensus selection identified {len(consensus_features)} optimal features")
    print(f"   6. Data is optimally balanced and ready for modeling")
    
    print(f"\nðŸ“‹ RECOMMENDATIONS FOR PHASE 2:")
    print(f"   1. Focus on AI Score features as primary predictors")
    print(f"   2. Consider ensemble methods to capture feature interactions")
    print(f"   3. Monitor for multicollinearity between AI Score features")
    print(f"   4. Use the prepared train/validation/test splits for modeling")
    print(f"   5. Consider regularization techniques for high-dimensional feature space")
    
except FileNotFoundError as e:
    print(f"âŒ Error loading analysis files: {e}")
    print("Please ensure all analysis scripts have been run successfully")

print(f"\n" + "=" * 60)
print("NEXT: Proceed to Phase 2 - Model Development")
print("=" * 60)