#!/usr/bin/env python3
"""
Phase 2: Final Data Preparation and Normalization
Complete the data preparation pipeline with proper scaling and encoding
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

print("=== PHASE 2: FINAL DATA PREPARATION AND NORMALIZATION ===")

# Load the final selected features datasets
X_train = pd.read_csv('pipeline_v4/data/X_train_final.csv').drop('target_binary', axis=1)
y_train = pd.read_csv('pipeline_v4/data/X_train_final.csv')['target_binary']
X_val = pd.read_csv('pipeline_v4/data/X_val_final.csv').drop('target_binary', axis=1)
y_val = pd.read_csv('pipeline_v4/data/X_val_final.csv')['target_binary']
X_test = pd.read_csv('pipeline_v4/data/X_test_final.csv').drop('target_binary', axis=1)
y_test = pd.read_csv('pipeline_v4/data/X_test_final.csv')['target_binary']

print(f"Training data: {X_train.shape}")
print(f"Validation data: {X_val.shape}")
print(f"Test data: {X_test.shape}")

# 1. FEATURE TYPE ANALYSIS
print("\n=== 1. FEATURE TYPE ANALYSIS ===")

def analyze_feature_types(df):
    """Analyze and classify feature types"""
    numeric_features = []
    binary_features = []
    categorical_features = []
    high_variance_features = []
    skewed_features = []
    
    for col in df.columns:
        unique_vals = df[col].nunique()
        
        if unique_vals <= 2:
            binary_features.append(col)
        elif df[col].dtype in ['int64', 'float64']:
            # Check for high variance (>1000)
            if df[col].var() > 1000:
                high_variance_features.append(col)
            # Check for skewness (>2 or <-2)
            if abs(df[col].skew()) > 2:
                skewed_features.append(col)
            numeric_features.append(col)
        else:
            categorical_features.append(col)
    
    return {
        'numeric': numeric_features,
        'binary': binary_features,
        'categorical': categorical_features,
        'high_variance': high_variance_features,
        'skewed': skewed_features
    }

feature_types = analyze_feature_types(X_train)

print(f"Feature breakdown:")
print(f"   Numeric features: {len(feature_types['numeric'])}")
print(f"   Binary features: {len(feature_types['binary'])}")
print(f"   Categorical features: {len(feature_types['categorical'])}")
print(f"   High variance features: {len(feature_types['high_variance'])}")
print(f"   Skewed features: {len(feature_types['skewed'])}")

# Show top 10 features by type
print(f"\nTop 10 numeric features:")
for i, feature in enumerate(feature_types['numeric'][:10], 1):
    print(f"   {i:2d}. {feature}")

# 2. SCALING STRATEGY DESIGN
print("\n=== 2. SCALING STRATEGY DESIGN ===")

# Define scaling strategies based on feature characteristics
scaling_strategy = {}

# High variance features -> RobustScaler or PowerTransformer
for feature in feature_types['high_variance']:
    scaling_strategy[feature] = 'robust'

# Skewed features -> PowerTransformer (Yeo-Johnson)
for feature in feature_types['skewed']:
    if feature not in scaling_strategy:
        scaling_strategy[feature] = 'power'

# Binary features -> No scaling needed
for feature in feature_types['binary']:
    scaling_strategy[feature] = 'none'

# Regular numeric features -> StandardScaler
for feature in feature_types['numeric']:
    if feature not in scaling_strategy:
        scaling_strategy[feature] = 'standard'

print(f"Scaling strategy defined for {len(scaling_strategy)} features")

# 3. APPLY SCALING
print("\n=== 3. APPLYING SCALING ===")

# Create scalers dictionary
scalers = {
    'standard': StandardScaler(),
    'robust': RobustScaler(),
    'power': PowerTransformer(method='yeo-johnson'),
    'minmax': MinMaxScaler()
}

# Apply scaling to each feature based on strategy
X_train_scaled = X_train.copy()
X_val_scaled = X_val.copy()
X_test_scaled = X_test.copy()

scaling_applied = {}

for feature in X_train.columns:
    strategy = scaling_strategy.get(feature, 'standard')
    
    if strategy != 'none':
        # Fit on training data
        scalers[strategy].fit(X_train[[feature]])
        
        # Transform all datasets
        X_train_scaled[feature] = scalers[strategy].transform(X_train[[feature]]).flatten()
        X_val_scaled[feature] = scalers[strategy].transform(X_val[[feature]]).flatten()
        X_test_scaled[feature] = scalers[strategy].transform(X_test[[feature]]).flatten()
        
        scaling_applied[feature] = strategy
    else:
        scaling_applied[feature] = 'none'

print(f"Scaling applied to {len(scaling_applied)} features")

# 4. FEATURE STATISTICS COMPARISON
print("\n=== 4. FEATURE STATISTICS COMPARISON ===")

def compare_statistics(original, scaled, feature_name):
    """Compare statistics before and after scaling"""
    return {
        'feature': feature_name,
        'original_mean': original[feature_name].mean(),
        'scaled_mean': scaled[feature_name].mean(),
        'original_std': original[feature_name].std(),
        'scaled_std': scaled[feature_name].std(),
        'original_min': original[feature_name].min(),
        'scaled_min': scaled[feature_name].min(),
        'original_max': original[feature_name].max(),
        'scaled_max': scaled[feature_name].max(),
        'scaling_method': scaling_applied[feature_name]
    }

# Compare statistics for top 10 features
comparison_stats = []
for feature in X_train.columns[:10]:
    stats = compare_statistics(X_train, X_train_scaled, feature)
    comparison_stats.append(stats)

comparison_df = pd.DataFrame(comparison_stats)
print("Top 10 features scaling comparison:")
print(comparison_df[['feature', 'original_mean', 'scaled_mean', 'original_std', 'scaled_std', 'scaling_method']].round(3))

# 5. OUTLIER DETECTION AND TREATMENT
print("\n=== 5. OUTLIER DETECTION AND TREATMENT ===")

def detect_outliers_iqr(df, feature):
    """Detect outliers using IQR method"""
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
    return len(outliers), lower_bound, upper_bound

# Detect outliers in top features
outlier_summary = []
for feature in X_train_scaled.columns[:10]:
    outlier_count, lower, upper = detect_outliers_iqr(X_train_scaled, feature)
    outlier_summary.append({
        'feature': feature,
        'outlier_count': outlier_count,
        'outlier_percentage': (outlier_count / len(X_train_scaled)) * 100,
        'lower_bound': lower,
        'upper_bound': upper
    })

outlier_df = pd.DataFrame(outlier_summary)
print("Outlier detection summary (top 10 features):")
print(outlier_df[['feature', 'outlier_count', 'outlier_percentage']].round(2))

# 6. FEATURE DISTRIBUTION ANALYSIS
print("\n=== 6. FEATURE DISTRIBUTION ANALYSIS ===")

# Calculate distribution metrics
distribution_metrics = []
for feature in X_train_scaled.columns:
    feature_data = X_train_scaled[feature]
    
    metrics = {
        'feature': feature,
        'mean': feature_data.mean(),
        'std': feature_data.std(),
        'skewness': feature_data.skew(),
        'kurtosis': feature_data.kurtosis(),
        'range': feature_data.max() - feature_data.min(),
        'cv': feature_data.std() / feature_data.mean() if feature_data.mean() != 0 else np.inf
    }
    distribution_metrics.append(metrics)

distribution_df = pd.DataFrame(distribution_metrics)

# Identify well-distributed vs problematic features
good_features = distribution_df[
    (abs(distribution_df['skewness']) < 2) & 
    (abs(distribution_df['kurtosis']) < 7) &
    (distribution_df['cv'] < 5)
]['feature'].tolist()

problematic_features = distribution_df[
    (abs(distribution_df['skewness']) >= 2) | 
    (abs(distribution_df['kurtosis']) >= 7) |
    (distribution_df['cv'] >= 5)
]['feature'].tolist()

print(f"Well-distributed features: {len(good_features)}")
print(f"Problematic features: {len(problematic_features)}")

# 7. FEATURE CORRELATION ANALYSIS
print("\n=== 7. FEATURE CORRELATION ANALYSIS ===")

# Calculate correlation matrix
correlation_matrix = X_train_scaled.corr()

# Find highly correlated pairs
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > 0.9:  # High correlation threshold
            high_corr_pairs.append({
                'feature1': correlation_matrix.columns[i],
                'feature2': correlation_matrix.columns[j],
                'correlation': corr_value
            })

if high_corr_pairs:
    high_corr_df = pd.DataFrame(high_corr_pairs)
    print(f"High correlation pairs found: {len(high_corr_pairs)}")
    print("Top 5 highest correlations:")
    print(high_corr_df.head()[['feature1', 'feature2', 'correlation']].round(3))
else:
    print("No highly correlated pairs found (threshold > 0.9)")

# 8. FINAL DATASET PREPARATION
print("\n=== 8. FINAL DATASET PREPARATION ===")

# Create final datasets
train_final = pd.concat([X_train_scaled, y_train], axis=1)
val_final = pd.concat([X_val_scaled, y_val], axis=1)
test_final = pd.concat([X_test_scaled, y_test], axis=1)

# Save final datasets
train_final.to_csv('pipeline_v4/data/X_train_preprocessed.csv', index=False)
val_final.to_csv('pipeline_v4/data/X_val_preprocessed.csv', index=False)
test_final.to_csv('pipeline_v4/data/X_test_preprocessed.csv', index=False)

print(f"‚úÖ Final preprocessed datasets saved:")
print(f"   Training: {train_final.shape}")
print(f"   Validation: {val_final.shape}")
print(f"   Test: {test_final.shape}")

# 9. CREATE COMPREHENSIVE PREPROCESSING REPORT
preprocessing_report = {
    'original_features': X_train.shape[1],
    'final_features': X_train_scaled.shape[1],
    'scaling_methods_applied': len([k for k, v in scaling_applied.items() if v != 'none']),
    'features_not_scaled': len([k for k, v in scaling_applied.items() if v == 'none']),
    'high_variance_features': len(feature_types['high_variance']),
    'skewed_features': len(feature_types['skewed']),
    'well_distributed_features': len(good_features),
    'problematic_features': len(problematic_features),
    'high_correlation_pairs': len(high_corr_pairs),
    'outlier_features_analyzed': len(outlier_df),
    'training_samples': len(train_final),
    'validation_samples': len(val_final),
    'test_samples': len(test_final),
    'target_balance_train': f"{y_train.mean():.1%} positive",
    'target_balance_val': f"{y_val.mean():.1%} positive",
    'target_balance_test': f"{y_test.mean():.1%} positive"
}

# Save preprocessing report
import json
with open('pipeline_v4/data/preprocessing_report.json', 'w') as f:
    json.dump(preprocessing_report, f, indent=2)

print(f"‚úÖ Preprocessing report saved to: pipeline_v4/data/preprocessing_report.json")

# 10. CREATE VISUALIZATIONS
print("\n=== 9. CREATING VISUALIZATIONS ===")

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Scaling method distribution
scaling_counts = pd.Series(scaling_applied).value_counts()
axes[0,0].pie(scaling_counts.values, labels=scaling_counts.index, autopct='%1.1f%%', startangle=90)
axes[0,0].set_title('Scaling Methods Distribution')

# Feature distribution quality
distribution_counts = pd.Series(['Well-distributed', 'Problematic']).map({
    'Well-distributed': len(good_features),
    'Problematic': len(problematic_features)
})
axes[0,1].bar(distribution_counts.index, distribution_counts.values, color=['green', 'red'], alpha=0.7)
axes[0,1].set_title('Feature Distribution Quality')
axes[0,1].set_ylabel('Number of Features')

# Outlier percentage distribution
if len(outlier_df) > 0:
    axes[0,2].hist(outlier_df['outlier_percentage'], bins=10, alpha=0.7, color='orange')
    axes[0,2].set_title('Outlier Percentage Distribution')
    axes[0,2].set_xlabel('Outlier Percentage')
    axes[0,2].set_ylabel('Frequency')

# Feature correlation heatmap (sample)
if len(X_train_scaled.columns) > 10:
    sample_features = np.random.choice(X_train_scaled.columns, 10, replace=False)
    sample_corr = X_train_scaled[sample_features].corr()
    sns.heatmap(sample_corr, annot=True, cmap='coolwarm', center=0, ax=axes[1,0], 
                square=True, fmt='.2f')
    axes[1,0].set_title('Sample Feature Correlation Matrix')

# Dataset size comparison
dataset_sizes = pd.Series([len(train_final), len(val_final), len(test_final)], 
                         index=['Train', 'Validation', 'Test'])
axes[1,1].bar(dataset_sizes.index, dataset_sizes.values, color=['blue', 'orange', 'green'], alpha=0.7)
axes[1,1].set_title('Dataset Size Distribution')
axes[1,1].set_ylabel('Number of Samples')

# Target distribution
target_dist = pd.Series([y_train.mean(), y_val.mean(), y_test.mean()], 
                       index=['Train', 'Validation', 'Test'])
axes[1,2].bar(target_dist.index, target_dist.values, color=['lightblue', 'lightcoral'], alpha=0.7)
axes[1,2].set_title('Target Distribution (Positive Rate)')
axes[1,2].set_ylabel('Positive Rate')
axes[1,2].set_ylim(0, 1)

plt.tight_layout()
plt.savefig('pipeline_v4/plots/preprocessing_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"‚úÖ Preprocessing analysis visualization saved to: pipeline_v4/plots/preprocessing_analysis.png")

print(f"\n=== FINAL DATA PREPARATION COMPLETE ===")
print(f"‚úÖ Original features: {X_train.shape[1]}")
print(f"‚úÖ Final features: {X_train_scaled.shape[1]}")
print(f"‚úÖ Training samples: {len(train_final)}")
print(f"‚úÖ Validation samples: {len(val_final)}")
print(f"‚úÖ Test samples: {len(test_final)}")
print(f"‚úÖ Preprocessing pipeline complete!")

# Print key findings
print(f"\nüîç KEY FINDINGS:")
print(f"   ‚Ä¢ {len(feature_types['high_variance'])} high-variance features normalized")
print(f"   ‚Ä¢ {len(feature_types['skewed'])} skewed features transformed")
print(f"   ‚Ä¢ {len(good_features)} well-distributed features identified")
print(f"   ‚Ä¢ {len(high_corr_pairs)} high-correlation pairs found")
print(f"   ‚Ä¢ {len(outlier_df)} features analyzed for outliers")
print(f"   ‚Ä¢ All datasets maintain target balance (~52.5% positive)")